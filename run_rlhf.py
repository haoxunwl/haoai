"""
ç›´æ¥è¿è¡ŒRLHFè®­ç»ƒ
"""

import os
import json
import time
import random
from typing import Dict, Any, List, Optional

import torch
from tqdm import tqdm

# è‡ªå®šä¹‰æ¨¡å—
from model.model import SmartHaoAI, HaoAIConfig
from model.reward_model import RewardModel
from simple_tokenizer import SimpleBPETokenizer as BPETokenizer
from train.rlhf_config import get_rlhf_config, RLHFConfig
from train.reward_trainer import train_reward_model
from train.ppo_trainer import create_ppo_trainer

# è®­ç»ƒå™¨ç±»
class RLHFTrainer:
    """RLHFè®­ç»ƒå™¨ï¼ˆç¨³å®šå¢å¼ºç‰ˆï¼‰"""

    def __init__(self, config_preset: str = "balanced"):
        self.config: RLHFConfig = get_rlhf_config(config_preset)

        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu"
        )

        self.tokenizer: Optional[BPETokenizer] = None
        self.policy_model: Optional[SmartHaoAI] = None
        self.reward_model: Optional[RewardModel] = None
        self.ppo_trainer = None

        self.current_stage = "init"
        self.training_stats: Dict[str, Any] = {}

    # ========================
    # ç¯å¢ƒåˆå§‹åŒ–
    # ========================
    def setup_environment(self) -> bool:
        print("\n[RLHF] åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ")
        print("=" * 60)
        print(f"é…ç½®é¢„è®¾: {self.config}")
        print(f"è¿è¡Œè®¾å¤‡: {self.device}")

        os.makedirs(self.config.rlhf_model_dir, exist_ok=True)
        os.makedirs(self.config.reward_model_dir, exist_ok=True)

        self.tokenizer = self._load_tokenizer()
        if self.tokenizer is None:
            return False

        self.policy_model = self._load_policy_model()
        if self.policy_model is None:
            return False

        # æ¨¡å‹å·²ç»åœ¨_load_policy_modelæ–¹æ³•ä¸­ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ä¸Šäº†
        # self.policy_model.to(self.device)
        self.policy_model.train()

        print("[SUCCESS] ç¯å¢ƒå‡†å¤‡å®Œæˆ")
        return True

    def _find_project_root(self) -> str:
        return os.path.abspath(
            os.path.dirname(__file__)
        )

    def _load_tokenizer(self) -> Optional[BPETokenizer]:
        project_root = self._find_project_root()

        candidates = [
            "weight/tokenizer/tokenizer.json",
            "weight/tokenizer.json",
            "tokenizer.json"
        ]

        for rel in candidates:
            path = os.path.join(project_root, rel)
            if os.path.exists(path):
                print(f"[LOAD] tokenizer: {path}")
                return BPETokenizer(path)

        print("[ERROR] æœªæ‰¾åˆ° tokenizer.json")
        return None

    def _load_policy_model(self) -> Optional[SmartHaoAI]:
        print("[LOAD] ç­–ç•¥æ¨¡å‹ï¼ˆSFTï¼‰")

        if os.path.exists(self.config.sft_model_dir):
            try:
                # ç›´æ¥åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
                model = SmartHaoAI.from_pretrained(
                    self.config.sft_model_dir,
                    device_map={"": self.device}
                )
                print("[SUCCESS] åŠ è½½ SFT æ¨¡å‹")
                return model
            except Exception as e:
                print(f"[WARN] SFT åŠ è½½å¤±è´¥: {e}")
                
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå°è¯•åˆ›å»ºæ–°æ¨¡å‹
                print("[INFO] åˆ›å»ºæ–°ç­–ç•¥æ¨¡å‹")
                config = HaoAIConfig(
                    vocab_size=self.tokenizer.vocab_size,
                    n_layer=8,
                    n_head=8,
                    n_embd=1024
                )
                model = SmartHaoAI(config)
                model.to(self.device)
                return model

        print("[INFO] åˆ›å»ºæ–°ç­–ç•¥æ¨¡å‹")

        config = HaoAIConfig(
            vocab_size=self.tokenizer.vocab_size,
            n_layer=8,
            n_head=8,
            n_embd=1024
        )
        model = SmartHaoAI(config)
        model.to(self.device)
        return model

    # ========================
    # å¥–åŠ±æ¨¡å‹
    # ========================
    def train_reward_model(self) -> bool:
        if not self.config.enable_reward_training:
            print("[SKIP] å¥–åŠ±æ¨¡å‹è®­ç»ƒè¢«ç¦ç”¨")
            return True

        print("\n[Stage 1] è®­ç»ƒå¥–åŠ±æ¨¡å‹")
        print("-" * 40)

        if not os.path.exists(self.config.preference_data_file):
            print("[ERROR] åå¥½æ•°æ®ä¸å­˜åœ¨")
            return False

        trainer = train_reward_model(
            tokenizer=self.tokenizer,
            data_file=self.config.preference_data_file,
            config=self.config.reward_config,
            save_dir=self.config.reward_model_dir
        )

        if trainer is None:
            return False

        self.reward_model = trainer.reward_model.to(self.device)
        self.reward_model.eval()

        print("[SUCCESS] å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return True

    def load_reward_model(self) -> bool:
        print("[LOAD] å¥–åŠ±æ¨¡å‹")

        for name in ["best_model", "final_model"]:
            path = os.path.join(self.config.reward_model_dir, name)
            if os.path.exists(path):
                self.reward_model = RewardModel.from_pretrained(path)
                self.reward_model.to(self.device).eval()
                print(f"[SUCCESS] ä½¿ç”¨å¥–åŠ±æ¨¡å‹: {name}")
                return True

        print("[ERROR] æœªæ‰¾åˆ°å¥–åŠ±æ¨¡å‹")
        return False

    # ========================
    # PPO è®­ç»ƒ
    # ========================
    def train_with_ppo(self) -> bool:
        print("\n[Stage 2] PPO å¼ºåŒ–å­¦ä¹ ")
        print("-" * 40)

        if self.reward_model is None:
            if not self.load_reward_model():
                return False

        self.ppo_trainer = create_ppo_trainer(
            self.policy_model,
            self.reward_model,
            self.tokenizer,
            self.config.ppo_config,
            device=self.device
        )

        return self._run_ppo_loop()

    def _sample_prompts(self, pool: List[str], k: int) -> List[str]:
        return random.sample(pool, k=min(k, len(pool)))

    def _run_ppo_loop(self) -> bool:
        prompts_pool = [
            "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
            "æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆ",
            "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿç®—æ³•çš„åŒºåˆ«",
            "è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨",
            "ç¥ç»ç½‘ç»œå¦‚ä½•å·¥ä½œ",
            "å¤§æ¨¡å‹çš„ä¼˜åŠ¿ä¸å±€é™",
            "å¼ºåŒ–å­¦ä¹ çš„åŸç†",
            "AI å¯¹ç¤¾ä¼šçš„å½±å“",
            "å¦‚ä½•è¯„ä»·ä¸€ä¸ªè¯­è¨€æ¨¡å‹",
            "æœªæ¥ AI çš„å‘å±•è¶‹åŠ¿"
        ]

        total_steps = self.config.ppo_config.total_timesteps
        save_freq = self.config.ppo_config.save_frequency

        pbar = tqdm(range(total_steps), desc="PPO Training")

        for step in pbar:
            try:
                prompts = self._sample_prompts(prompts_pool, k=4)

                rollouts = self.ppo_trainer.collect_rollouts(prompts)
                if not rollouts:
                    continue

                loss_info = self.ppo_trainer.train_step(rollouts)

                if loss_info:
                    pbar.set_postfix({
                        "reward": f"{loss_info.get('mean_reward', 0):.3f}",
                        "policy": f"{loss_info.get('policy_loss', 0):.3f}",
                        "value": f"{loss_info.get('value_loss', 0):.3f}",
                    })

                if step % save_freq == 0 and step > 0:
                    self._save_checkpoint(step)

            except Exception as e:
                print(f"[WARN] step {step} å¤±è´¥: {e}")

        print("[SUCCESS] PPO è®­ç»ƒå®Œæˆ")
        return True

    # ========================
    # ä¿å­˜ & è¯„ä¼°
    # ========================
    def _save_checkpoint(self, step: int):
        path = os.path.join(
            self.config.rlhf_model_dir,
            f"checkpoint_{step}"
        )
        os.makedirs(path, exist_ok=True)

        self.policy_model.save_pretrained(path)

        with open(os.path.join(path, "info.json"), "w", encoding="utf-8") as f:
            json.dump({
                "step": step,
                "time": time.time()
            }, f, indent=2, ensure_ascii=False)

        print(f"[SAVE] checkpoint @ step {step}")

    def train(self) -> bool:
        if not self.setup_environment():
            return False

        self.current_stage = "reward"
        if self.config.enable_reward_training:
            if not self.train_reward_model():
                return False
        else:
            if not self.load_reward_model():
                return False

        self.current_stage = "ppo"
        if self.config.enable_ppo_training:
            if not self.train_with_ppo():
                return False

        self._save_final_model()
        print("\nğŸ‰ RLHF è®­ç»ƒæµç¨‹å®Œæˆ")
        return True

    def _save_final_model(self):
        path = os.path.join(self.config.rlhf_model_dir, "final_model")
        os.makedirs(path, exist_ok=True)

        self.policy_model.save_pretrained(path)

        with open(os.path.join(path, "summary.json"), "w", encoding="utf-8") as f:
            json.dump({
                "timestamp": time.time(),
                "architecture": "HaoAI-RLHF",
                "device": str(self.device)
            }, f, indent=2, ensure_ascii=False)

        print(f"[SAVE] æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³ {path}")

# ä¸»å‡½æ•°
def main():
    print("ç›´æ¥è¿è¡ŒRLHFè®­ç»ƒ")
    print("=" * 50)
    
    trainer = RLHFTrainer(config_preset="balanced")
    ok = trainer.train()

    if ok:
        print("[SUCCESS] è®­ç»ƒæˆåŠŸ")
    else:
        print("[FAILED] è®­ç»ƒå¤±è´¥")

# è¿è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    main()